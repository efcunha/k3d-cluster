#!/bin/sh

# Instalation variables
CLUSTER_DOMAIN=<COLOCAR O DOMINIO>
API_PORT=6443
CLUSTER_NAME=k3d-rancher
READ_VALUE=
SERVERS=1
AGENTS=1
TRAEFIK_V2=Yes
TRAEFIK_LABEL="app.kubernetes.io/instance=traefik"
INSTALL_PVP=Yes
INSTALL_CERTMANAGER=Yes
INSTALL_NGINX=Yes
INSTALL_TRAEFIK=No
INSTALL_RANCHER=Yes
INSTALL_JENKINS=No
INSTALL_SONARQUBE=No
INSTALL_HARBOR=No
INSTALL_NEUVECTOR=No
INSTALL_DASHBOARD=No
INSTALL_PROMETHEUS=No

# bold text 
bold=$(tput bold)
normal=$(tput sgr0)
yes_no="(${bold}Y${normal}es/${bold}N${normal}o)"


# $1 text to show - $2 default value
read_value ()
{
    read -p "${1} [${bold}${2}${normal}]: " READ_VALUE
    if [ "${READ_VALUE}" = "" ]
    then
        READ_VALUE=$2
    fi
}

# Check if exist docker, k3d and kubectl
checkDependencies ()
{
    # Check Docker
    if ! type docker > /dev/null; then
        echo "Docker could not be found. Installing it ..."
        curl -L -o ./install-docker.sh "https://get.docker.com"
        chmod +x ./install-docker.sh
        ./install-docker.sh
        sudo usermod -aG docker $USER
        #exit
    fi

    # Check K3D
    if ! type k3d > /dev/null; then
        echo "K3D could not be found. Installing it ..."
        curl -s https://raw.githubusercontent.com/rancher/k3d/main/install.sh | bash
        # Install k3d autocompletion for bash
        echo "source <(k3d completion bash)" >> ~/.bashrc
        #exit
    fi

    # Check Kubectl
    if ! type kubectl > /dev/null; then
        echo "Kubectl could not be found. Installing it ..."
        curl -LO "https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl"
        chmod +x ./kubectl
        sudo mv ./kubectl /usr/local/bin/kubectl
        kubectl version --client
        #exit
    fi

    # Check Helm
    if ! type helm > /dev/null; then
        echo "Helm could not be found. Installing it ..."
        curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3
        chmod +x ./get_helm.sh
        ./get_helm.sh
        # Add default repos
        helm repo add stable https://charts.helm.sh/stable
        # Add kubernetes-dashboard repository
        helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/
        # Add bitnami helm repos
        helm repo add bitnami https://charts.bitnami.com/bitnami
        # Add Prometheus helm repos
        helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
        # Add Rancher helm repos
        helm repo add rancher-latest https://releases.rancher.com/server-charts/latest
        # Add jetstack helm repos
	    helm repo add jetstack https://charts.jetstack.io
        # Add Jenkins helm repos
	    helm repo add jenkins https://raw.githubusercontent.com/jenkinsci/kubernetes-operator/master/chart
		# Add Harbor helm repos
		helm repo add harbor https://helm.goharbor.io
		# Add Neuvector helm repos
		helm repo add neuvector https://neuvector.github.io/neuvector-helm/
	    # Update helm
        helm repo update
        #exit
    fi
}

header()
{
    echo "\n\n${bold}${1}${normal}\n-------------------------------------"
}

footer()
{
    echo "-------------------------------------"
}

configValues ()
{
  read_value "Nome Cluster" "${CLUSTER_NAME}"
  CLUSTER_NAME=${READ_VALUE}
  read_value "Cluster Domain" "${CLUSTER_DOMAIN}"
  CLUSTER_DOMAIN=${READ_VALUE}
  read_value "Porta API" "${API_PORT}"
  API_PORT=${READ_VALUE}
  read_value "Servers (Masters)" "${SERVERS}"
  SERVERS=${READ_VALUE}
  read_value "Agents (Workers)" "${AGENTS}"
  AGENTS=${READ_VALUE}
}

installCluster ()
{
  header "Deletando Previous Cluster"
  k3d cluster delete ${CLUSTER_NAME}
  footer

  header "Criando K3D cluster"
#https://github.com/rancher/k3d/blob/main/tests/assets/config_test_simple.yaml
  cat <<EOF  > tmp-k3d-${CLUSTER_NAME}.yaml
apiVersion: k3d.io/v1alpha2
kind: Simple
name: ${CLUSTER_NAME}
servers: ${SERVERS} 
agents: ${AGENTS}
kubeAPI:
  hostIP: "0.0.0.0"
  hostPort: "${API_PORT}" # kubernetes api port 6443:6443

image: rancher/k3s:latest

volumes:
  - volume: $(pwd)/k3dvol:/k3dvol # volume in host:container
    nodeFilters:
      - all

ports:
  - port: 0.0.0.0:80:80
    nodeFilters:
      - loadbalancer
  - port: 0.0.0.0:443:443
    nodeFilters:
      - loadbalancer
  - port: 0.0.0.0:8900:30080
    nodeFilters:
      - loadbalancer
  - port: 0.0.0.0:8901:30081
    nodeFilters:
      - loadbalancer
  - port: 0.0.0.0:8902:30082
    nodeFilters:
      - loadbalancer
env:
  - envVar: secret=token
    nodeFilters:
      - all
labels:
  - label: best_cluster=forced_tag
    nodeFilters:
      - server[0]  
      - loadbalancer

options:
  k3d:
    wait: true
    timeout: "60s" # avoid an start/stop cicle when start fails
    disableLoadbalancer: false
    disableImageVolume: false
  k3s:
    extraServerArgs:
      - --tls-san=127.0.0.1
      - --no-deploy=traefik

    extraAgentArgs: []
  kubeconfig:
    updateDefaultKubeconfig: true # update kubeconfig when cluster starts
    switchCurrentContext: true # change this cluster context when cluster starts
EOF

  k3d cluster create --config tmp-k3d-${CLUSTER_NAME}.yaml

  header "waiting for cluster init ..."
  sleep 5

  kubectl config use-context k3d-${CLUSTER_NAME}
  kubectl cluster-info
  footer
}

installPVP ()
{
  header "Provisionando Volume Persistente"
  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolume
metadata:
  name: k3d-pv
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 50Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/k3dvol"
EOF

  cat <<EOF | kubectl delete -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: local-path-provisioner-role
rules:
  - apiGroups: [ "" ]
    resources: [ "nodes", "persistentvolumeclaims", "configmaps" ]
    verbs: [ "get", "list", "watch" ]
  - apiGroups: [ "" ]
    resources: [ "endpoints", "persistentvolumes", "pods" ]
    verbs: [ "*" ]
  - apiGroups: [ "" ]
    resources: [ "events" ]
    verbs: [ "create", "patch" ]
  - apiGroups: [ "storage.k8s.io" ]
    resources: [ "storageclasses" ]
    verbs: [ "get", "list", "watch" ]
EOF

  cat <<EOF | kubectl delete -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: local-path-provisioner-bind
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: local-path-provisioner-role
subjects:
  - kind: ServiceAccount
    name: local-path-provisioner-service-account
    namespace: local-path-storage
EOF

  cat <<EOF | kubectl delete -f -
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-path
provisioner: rancher.io/local-path
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Delete
EOF

#  kubectl describe pv k3d-pv
  curl -LO  https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
  sleep 5
  kubectl apply -f local-path-storage.yaml
  sleep 5
  kubectl patch storageclass local-path -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
  footer
}

installCertManager ()
{
  header "Instalando Cert-Manager"
  # Update System
  sudo apt-get update -y

  # Install dependenci
  sudo apt-get install wget libnss3-tools

  # Install Mkcert
  curl -JLO "https://dl.filippo.io/mkcert/latest?for=linux/amd64"
  chmod +x mkcert-v*-linux-amd64
  sudo cp mkcert-v*-linux-amd64 /usr/local/bin/mkcert
  mkcert -install
  mkcert -CAROOT

  # Criar certificados
  mkcert ${CLUSTER_DOMAIN} localhost 127.0.0.1 192.168.1.10 ::1
  mv *-key.pem certs/${CLUSTER_DOMAIN}-key.pem
  mv *.pem certs/${CLUSTER_DOMAIN}.pem
  sleep 10

  # Create Namespace
  kubectl create namespace ingress

  # Create a secret with server certificate
  kubectl --namespace ingress create secret tls server-certs --key certs/${CLUSTER_DOMAIN}-key.pem --cert certs/${CLUSTER_DOMAIN}.pem
  sleep 5

  kubectl apply --validate=false -f https://github.com/cert-manager/cert-manager/releases/download/v1.8.2/cert-manager.crds.yaml
  sleep 5

  kubectl create namespace cert-manager
  sleep 5

  helm install \
  cert-manager jetstack/cert-manager \
    --namespace cert-manager \
    --version v1.8.2
  sleep 20

  kubectl create namespace cattle-system
  sleep 5
}

installTraefik ()
{
  header "Instalando Traefik Ingress"
  # Install Traefik V2
  helm install traefik traefik/traefik
  sleep 20
  footer
  # Expose Traefik Dashboard
  kubectl expose deploy/traefik -n default --port=9000 --target-port=9000 --name=traefik-dashboard
  kubectl create ingress traefik-dashboard --rule="traefik.${CLUSTER_DOMAIN}/*=traefik-dashboard:9000"
  sleep 5
  echo "You can reach Traefik dashbaord by: http://localhost/dashboard/ ou  http://traefik.dominio.com.br/dashboard/"
  footer
}

installIngress ()
{
  header "Instalando Nginx Ingress"
  kubectl -n ingress create secret tls tls-nginx-ingress --cert=/certs/tls.crt --key=/certs/tls.key 
  kubectl -n ingress create secret generic tls-ca --from-file=/certscacerts.pem
  
  # Install ingress with tls enabled providing certificates stored in namespace
  cat <<EOF | helm install --namespace ingress -f - ingress bitnami/nginx-ingress-controller
extraArgs:
  default-ssl-certificate: "ingress/tls-nginx-ingress"
EOF
  footer
  sleep 5
  header "LoadBalancer info:"
  kubectl -n ingress get svc | egrep -e NAME -e LoadBalancer
  sleep 20
  echo "acesso ao Nginx https://127.0.0.1 ou https://nginx.xxxx.com"
  footer
}

installRancher ()
{
  header "Instalando Rancher UI"
  
  kubectl -n cattle-system create secret tls tls-rancher-ingress --cert=/certs/tls.crt --key=/certs/tls.key 
  kubectl -n cattle-system create secret generic tls-ca --from-file=/certscacerts.pem
  
  cat <<EOF | helm --install rancher rancher-stable/rancher \
   --namespace cattle-system \
   --set hostname=rancher.$CLUSTER_DOMAIN \
   --set ingress.tls.source=tls-rancher-ingress \
   --set privateCA=true \
   --set replicas=1
EOF

  sleep 5
  kubectl -n cattle-system rollout status deploy/rancher
  kubectl -n cattle-system get deploy rancher
  echo "acesso ao Rancher https://127.0.0.1/ ou https://rancher.xxxx.com/"
  footer
}

installJenkins ()
{
  header "Instalando Jenkins"
    sleep 5
  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Namespace
metadata:
  name: jenkins
EOF

   sleep 5
  cat <<EOF | kubectl apply -f -
   apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
  managedFields:
  name: jenkins-pv
  namespace: jenkins
provisioner: rancher.io/local-path
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
EOF

  sleep 5
  cat <<EOF | kubectl apply -f -
  apiVersion: v1
kind: PersistentVolume
metadata:
  name: jenkins-pv
  namespace: jenkins
spec:
  storageClassName: jenkins-pv
  accessModes:
    - ReadWriteOnce
  capacity:
    storage: 20Gi
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: /data/jenkins-volume/
EOF

  sleep 5
  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: jenkins
  namespace: jenkins
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: jenkins
  namespace: jenkins
rules:
- apiGroups:
  - '*'
  resources:
  - statefulsets
  - services
  - replicationcontrollers
  - replicasets
  - podtemplates
  - podsecuritypolicies
  - pods
  - pods/log
  - pods/exec
  - podpreset
  - poddisruptionbudget
  - persistentvolumes
  - persistentvolumeclaims
  - jobs
  - endpoints
  - deployments
  - deployments/scale
  - daemonsets
  - cronjobs
  - configmaps
  - namespaces
  - events
  - secrets
  verbs:
  - create
  - get
  - watch
  - delete
  - list
  - patch
  - update
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - get
  - list
  - watch
  - update
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: jenkins
  namespace: jenkins
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: jenkins
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:serviceaccounts:jenkins
EOF

sleep 5
kubectl -n jenkins create secret tls tls-jenkins-ingress --cert=../certs/tls.crt --key=../certs/tls.key
kubectl -n jenkins create secret generic tls-ca --from-file=../certs/cacerts.pem

chart=jenkinsci/jenkins
helm install jenkins -n jenkins -f jenkins-values.yaml $chart

  cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
  managedFields:
  name: jenkinsingress
  namespace: jenkins
spec:
  ingressClassName: nginx
  rules:
  - host: jenkins.edson-devops.eti.br
    http:
      paths:
      - backend:
          service:
            name: jenkins
            port:
              number: 8080
        path: /
        pathType: Prefix
  tls:
  - hosts:
    - jenkins.edson-devops.eti.br
    secretName: tls-jenkins-ingress
EOF

sleep 5
footer
jsonpath="{.data.jenkins-admin-password}"
secret=$(kubectl get secret -n jenkins jenkins -o jsonpath=$jsonpath)
echo $(echo $secret | base64 --decode)
footer
}

installHarbor ()

{
  header "Instalando Harbor"

  sleep 5
  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Namespace
metadata:
  name: harbor
EOF

  sleep 5
  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-pvc
  namespace: harbor
  annotations:
    volume.beta.kubernetes.io/storage-class: local-path
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: postgres
  namespace: harbor
  labels:
    app: postgres
spec:
  ports:
    - port: 5432
  selector:
    app: postgres
  clusterIP: None
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres
  namespace: harbor
  labels:
    app: postgres
spec:
  strategy:
    type: Recreate
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      schedulerName: stork
      containers:
      - name: postgres
        image: postgres:13.2
        readinessProbe:
          exec:
            command: ["psql", "-w", "-U", "postgres", "-c", "SELECT 1"]
          initialDelaySeconds: 15
          timeoutSeconds: 2
        livenessProbe:
          exec:
            command: ["psql", "-w", "-U", "postgres", "-c", "SELECT 1"]
          initialDelaySeconds: 45
          timeoutSeconds: 2
        ports:
        - containerPort: 5432
          name: postgres
        env:
        - name: POSTGRES_USER
          value: postgres
        - name: POSTGRES_PASSWORD
          value: password
        - name: PGDATA
          value: /var/lib/postgresql/data/pgdata
        volumeMounts:
        - name: postgres-persistent-storage
          mountPath: /var/lib/postgresql/data
      volumes:
      - name: postgres-persistent-storage
        persistentVolumeClaim:
          claimName: postgres-pvc
EOF

  sleep 5
  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: redis-pvc
  namespace: harbor
  annotations:
    volume.beta.kubernetes.io/storage-class: local-path
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: redis
  namespace: harbor
spec:
  ports:
    - port: 6379
      name: redis
  clusterIP: None
  selector:
    app: redis
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  namespace: harbor
  labels:
    app: redis
spec:
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      schedulerName: stork
      containers:
      - name: redis
        image: redis:3.2-alpine
        imagePullPolicy: Always
        args: ["--requirepass", "password"]
        ports:
          - containerPort: 6379
            name: redis
        volumeMounts:
          - name: redis-vol
            mountPath: /data
      volumes:
      - name: redis-vol
        persistentVolumeClaim:
          claimName: redis-pvc
EOF

sleep 5
DB_POD=$(kubectl get pods -n harbor -l app=postgres | awk '/postgres/{print$1}')
kubectl exec $DB_POD -n harbor -- createdb -Upostgres registry
kubectl exec $DB_POD -n harbor -- createdb -Upostgres clair
kubectl exec $DB_POD -n harbor -- createdb -Upostgres notary_server
kubectl exec $DB_POD -n harbor -- createdb -Upostgres notary_signer

sleep 5
kubectl -n harbor-system create secret tls tls-harbor-ingress --cert=../certs/tls.crt --key=../certs/tls.key
kubectl -n harbor-system create secret generic tls-ca --from-file=../certs/cacerts.pem

sleep 5
  cat <<EOF | helm install harbor harbor/harbor \
  --set redis.type=external \
  --set redis.external.addr=redis.harbor:6379 \
  --set redis.external.password=password \
  --set database.type=external \
  --set database.external.host=postgres.harbor \
  --set database.external.username=postgres \
  --set database.external.password=password \
  --set persistence.persistentVolumeClaim.registry.storageClass=local-path \
  --set persistence.persistentVolumeClaim.chartmuseum.storageClass=local-path \
  --set persistence.persistentVolumeClaim.jobservice.storageClass=local-path \
  --set persistence.persistentVolumeClaim.trivy.storageClass=local-path \
  --set expose.tls.enabled=true \
  --set expose.tls.secret.secretName=tls-harbor-ingress
  --set externalURL=https://harbor.$CLUSTER_DOMAIN \
  --set expose.ingress.hosts.core=ocr.$CLUSTER_DOMAIN \
  --set expose.ingress.hosts.notary=notary.$CLUSTER_DOMAIN \
  --namespace harbor
EOF
  
}

installNeuvector ()
{
  header "Instalando Neuvector"
  cat <<EOF | kubectl apply -f -
  apiVersion: v1
kind: Namespace
metadata:
  name: neuvector
EOF

sleep 5
kubectl -n neuvector create secret tls tls-neuvector-ingress --cert=../certs/tls.crt --key=../certs/tls.key
kubectl -n neuvector create secret generic tls-ca --from-file=../certs/cacerts.pem

sleep 5
helm upgrade --install neuvector --namespace neuvector neuvector/core -f neuvector-values.yaml
    
}

installSonarqube ()
{
  header "Instalando Sonarqube"
  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Namespace
metadata:
  name: sonarqube
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
  managedFields:
  name: sonarqube-pv
provisioner: rancher.io/local-path
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolume
metadata:
  name: sonarqube-pv
  namespace: sonarqube
spec:
  storageClassName: sonarqube-pv
  accessModes:
    - ReadWriteOnce
  capacity:
    storage: 10Gi
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: /data/sonarqube-volume/sonarqube
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolume
metadata:
  name: sonarqube-postgresql-pv
  namespace: sonarqube
spec:
  storageClassName: sonarqube-pv
  accessModes:
    - ReadWriteOnce
  capacity:
    storage: 10Gi
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: /data/sonarqube-volume/postgresql
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  annotations:
  finalizers:
  labels:
  managedFields:
  name: sonarqube
  namespace: sonarqube
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: sonarqube-pv
  volumeMode: Filesystem
  volumeName: sonarqube-pv
status:
  accessModes:
  - ReadWriteOnce
  capacity:
    storage: 20Gi
  phase: Bound
EOF

  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  annotations:
  finalizers:
  labels:
  managedFields:
  name: data-sonarqube-postgresql-0
  namespace: sonarqube
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 8Gi
  storageClassName: sonarqube-pv
  volumeMode: Filesystem
  volumeName: sonarqube-postgresql-pv
status:
  accessModes:
  - ReadWriteOnce
  capacity:
    storage: 20Gi
  phase: Bound
EOF

sleep 5
kubectl -n sonarqube create secret tls tls-sonarqube-ingress --cert=../certs/tls.crt --key=../certs/tls.key
kubectl -n sonarqube create secret generic tls-ca --from-file=../certs/cacerts.pem

sleep5
helm install sonarqube -n sonarqube bitnami/sonarqube -f sonarqube-ingress.yaml

sleep5
footer
echo Username: user
echo Password: $(kubectl get secret --namespace sonarqube sonarqube -o jsonpath="{.data.sonarqube-password}" | base64 -d)
footer
}


installDashboard ()
{
  header "Instalando Dashboard"
  # Install Kubernetes Dashboard
  kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml
  # Create dashboard account
  kubectl create serviceaccount dashboard-admin-sa
  # bind the dashboard-admin-service-account service account to the cluster-admin role
  kubectl create clusterrolebinding dashboard-admin-sa --clusterrole=cluster-admin --serviceaccount=default:dashboard-admin-sa

  # display token
  header "Keep this Token to acces dashboard"
  #kubectl describe secret $(kubectl get secrets | grep dashboard-admin-sa | cut -d' ' -f1)
  kubectl describe secret $(kubectl get secrets | grep dashboard-admin-sa | awk '{ print $1 }')
  header "Dashboard Access:"
  echo "kubectl proxy"
  echo "http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/login"
  footer
}


installPrometheus ()
{
  header "Instaling Prometheus & Grafana"
  # Install Prometheus
  helm install --namespace monitoring --create-namespace prometheus  \
  --set server.global.scrape_interval=30s prometheus-community/prometheus
  # Install Grafana
  helm install --namespace monitoring --create-namespace grafana stable/grafana \
  --set sidecar.datasources.enabled=true --set sidecar.dashboards.enabled=true \
  --set sidecar.datasources.label=grafana_datasource \
  --set sidecar.dashboards.label=grafana_dashboard
  # Create Ingress controler using server certificates
  cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: monitoring
  namespace: monitoring
  annotations:
    ingress.kubernetes.io/ssl-redirect: "true"
spec:
  rules:
    - host: grafana.${CLUSTER_DOMAIN}
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: grafana
                port:
                  number: 80
    - host: prometheus.${CLUSTER_DOMAIN}
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: prometheus-server
                port:
                  number: 80
EOF

  header "Grafana and Prometheus Access:"
  echo "${bold}Prometheus${normal}:"
  echo "url: https://prometheus.${CLUSTER_DOMAIN}"
  footer
  echo "${bold}Grafana${normal}:"
  echo "url: https://grafana.${CLUSTER_DOMAIN}"
  echo "username: admin"
  echo "password: $(kubectl get secret --namespace monitoring grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo)"
  echo "\nuse 'kubectl get secret --namespace monitoring grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo' to get password"
  footer
}

installKubeapps ()
{
  header "Instaling Kubeapps"
  cat <<EOF | helm install --namespace kubeapps --create-namespace -f - kubeapps bitnami/kubeapps
ingress:
  enabled: true
  hostname: kubeapps.${CLUSTER_DOMAIN}
  tls: true
EOF
  #helm install --namespace kubeapps --create-namespace kubeapps bitnami/kubeapps
  kubectl create serviceaccount kubeapps-operator
  kubectl create clusterrolebinding kubeapps-operator --clusterrole=cluster-admin --serviceaccount=default:kubeapps-operator

  echo "url: https://kubeapps.${CLUSTER_DOMAIN}"
  echo "Token: \n$(  kubectl get secret $(kubectl get serviceaccount kubeapps-operator -o jsonpath='{range .secrets[*]}{.name}{"\n"}{end}' | grep kubeapps-operator-token) -o jsonpath='{.data.token}' -o go-template='{{.data.token | base64decode}}' && echo)"
  footer
}

isSelected()
{
  if [ "${1}" = "Yes" ] || [ "${1}" = "yes" ] || [ "${1}" = "Y" ]  || [ "${1}" = "y" ];
  then
    echo 1
  else
    echo 0
  fi
}

installAddons ()
{

  read_value "Instalar Provisionando Volume Persistente" "${INSTALL_PVP}"
  if [ $(isSelected ${READ_VALUE}) = 1 ];
  then
      installPVP
  fi

  read_value "Instalar Cert-Manager? ${yes_no}" "${INSTALL_CERTMANAGER}"
  if [ $(isSelected ${READ_VALUE}) = 1 ];
  then
      installCertManager
  fi

  read_value "Instalar Traefik Ingress? ${yes_no}" "${INSTALL_TRAEFIK}"
  if [ $(isSelected ${READ_VALUE}) = 1 ];
  then
      installTraefik
  fi

  read_value "Instalar Nginx Ingress? ${yes_no}" "${INSTALL_NGINX}"
  if [ $(isSelected ${READ_VALUE}) = 1 ];
  then
      installIngress
  fi

  read_value "Instalar Rancher? ${yes_no}" "${INSTALL_RANCHER}"
  if [ $(isSelected ${READ_VALUE}) = 1 ];
  then
      installRancher
  fi

  read_value "Instalar Jenkins? ${yes_no}" "${INSTALL_JENKINS}"
  if [ $(isSelected ${READ_VALUE}) = 1 ];
  then
      installJenkins
  fi

  read_value "Instalar Sonarqube? ${yes_no}" "${INSTALL_SONARQUBE}"
  if [ $(isSelected ${READ_VALUE}) = 1 ];
  then
      installSonarqube
  fi
  
  read_value "Instalar Harbor? ${yes_no}" "${INSTALL_HARBOR}"
  if [ $(isSelected ${READ_VALUE}) = 1 ];
  then
      installHarbor
  fi

  read_value "Instalar Neuvector? ${yes_no}" "${INSTALL_NEUVECTOR}"
  if [ $(isSelected ${READ_VALUE}) = 1 ];
  then 
      installNeuvector
  fi
  
  read_value "Instalar Dashbard? ${yes_no}" "${INSTALL_DASHBOARD}"
  if [ $(isSelected ${READ_VALUE}) = 1 ];
  then
      installDashboard
  fi

  read_value "Instalar Prometheus? ${yes_no}" "${INSTALL_PROMETHEUS}"
  if [ $(isSelected ${READ_VALUE}) = 1 ];
  then
      installPrometheus
  fi
}

checkDependencies

#Retrieve config values
configValues

# Todo Calico

installCluster

installAddons

